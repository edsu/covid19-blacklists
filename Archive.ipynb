{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive\n",
    "\n",
    "Some of the notebooks here will examine the HTML served up by the domains identified by DomainTools. To make that work a bit faster we're going to create an archive of web content using the [WARC](https://en.wikipedia.org/wiki/Web_ARChive) format so we don't have to keep going back to the web to fetch the content. The [warcio](https://github.com/webrecorder/warcio) library lets us easily write and read WARC data, which is essentially a concatenation of all the web content along with HTTP request and response headers that document when and how it was requested. The HTTP metadata is actually pretty important for anlyzing where things are redirecting to. warcio works nicely with the [requests](https://requests.readthedocs.io/en/master/) that makes fetching the content easy too. So let's start by importing those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warcio.capture_http import capture_http\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the DomainTools data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>created</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>covid-19vacuna.net</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coronafiji.online</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thecoronacon.com</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covid19simplecottonmasksforhospitals.fail</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>covidplasmasavealife.com</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      domain    created  risk\n",
       "0                         covid-19vacuna.net 2020-04-06    99\n",
       "1                          coronafiji.online 2020-04-06    99\n",
       "2                           thecoronacon.com 2020-04-06    99\n",
       "3  covid19simplecottonmasksforhospitals.fail 2020-04-06    99\n",
       "4                   covidplasmasavealife.com 2020-04-06    99"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('data/domaintools/2020-04-07.csv.gz',\n",
    "    parse_dates=['created'], \n",
    "    sep='\\t',\n",
    "    names=['domain', 'created', 'risk']\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all we have is a domain we will need to fish around a little bit seeing if the website is http or https, and if a www prefix is needed. These two functions will do this fishing around.\n",
    "\n",
    "This one just fetches a URL with HTTP and returns a URL if the response was a 200 OK. If the response was not 200 OK, or some kind of excpetion was thrown it returns None. Since requests follows redirects the supplied URL could be different from the URL that is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(url):\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=5)\n",
    "        if resp.status_code == 200:\n",
    "            # return the final url after potential redirects \n",
    "            return resp.url\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can give `get_homepage` a domain and it will attempt to get the \"home page\" for that domain, by first trying an https protocol variant, then an http, and then doing the same again but with a \"www\" prefix on the domain. The first one to return content will cause the function to return the found URL. Note the found URL could be different than the URL that was checked because there could be HTTP redirects involved (often the case with malware)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_homepage(domain):  \n",
    "    urls = [\n",
    "        \"https://\" + domain,\n",
    "        \"http://\" + domain,\n",
    "        \"https://www.\" + domain,\n",
    "        \"http://www.\" + domain\n",
    "    ]\n",
    "        \n",
    "    for url in urls:\n",
    "        found_url = get(url)\n",
    "        if found_url:\n",
    "            return found_url\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to iterate through the domains, fetch them from the web, and write them to a WARC file that we are going to name using the current date. We are also going to keep track of the URL where we found content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to data/warc/domaintools-2020-04-08.warc.gz\n",
      "++--+++++++-++-++--++++++-+++-++++++-+-+-+++"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import datetime\n",
    "\n",
    "today = datetime.date.today().strftime('%Y-%m-%d')\n",
    "warc_file = pathlib.Path('data') / \"warc\" / \"domaintools-{}.warc.gz\".format(today)\n",
    "print(\"writing to {}\".format(warc_file))\n",
    "\n",
    "found = {}\n",
    "with capture_http(warc_file.as_posix()):\n",
    "    for domain in df.domain:\n",
    "        url = get_homepage(domain)\n",
    "        if url:\n",
    "            sys.stdout.write('+')\n",
    "        else:\n",
    "            sys.stdout.write('-')\n",
    "        found[domain] = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
